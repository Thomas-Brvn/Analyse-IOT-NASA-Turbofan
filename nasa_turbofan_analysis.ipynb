{
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA C-MAPSS Turbofan Engine Degradation Analysis\n",
    "\n",
    "Analyse du dataset NASA pour la prediction de la duree de vie utile restante (RUL) des moteurs turbofan.\n",
    "\n",
    "## Structure des donnees\n",
    "\n",
    "- **unit_id**: Identifiant du moteur\n",
    "- **cycle**: Cycle de fonctionnement\n",
    "- **op_setting_1, 2, 3**: Parametres operationnels\n",
    "- **sensor_1 to sensor_21**: Mesures des 21 capteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noms des colonnes\n",
    "columns = ['unit_id', 'cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3'] + \\\n",
    "          [f'sensor_{i}' for i in range(1, 22)]\n",
    "\n",
    "# Charger le dataset FD001 (plus simple pour commencer)\n",
    "train_df = pd.read_csv('data/CMaps/train_FD001.txt', sep='\\s+', header=None, names=columns)\n",
    "test_df = pd.read_csv('data/CMaps/test_FD001.txt', sep='\\s+', header=None, names=columns)\n",
    "rul_df = pd.read_csv('data/CMaps/RUL_FD001.txt', sep='\\s+', header=None, names=['RUL'])\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"RUL shape: {rul_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calcul du RUL (Remaining Useful Life)\n",
    "\n",
    "Pour les donnees d'entrainement, le RUL est calcule comme:\n",
    "**RUL = max_cycle - current_cycle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le RUL pour chaque observation dans le train set\n",
    "def calculate_rul(df):\n",
    "    # Trouver le cycle max pour chaque moteur\n",
    "    max_cycles = df.groupby('unit_id')['cycle'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_id', 'max_cycle']\n",
    "    \n",
    "    # Joindre et calculer le RUL\n",
    "    df = df.merge(max_cycles, on='unit_id')\n",
    "    df['RUL'] = df['max_cycle'] - df['cycle']\n",
    "    df.drop('max_cycle', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = calculate_rul(train_df)\n",
    "print(f\"Nombre de moteurs: {train_df['unit_id'].nunique()}\")\n",
    "print(f\"\\nDistribution du RUL:\")\n",
    "print(train_df['RUL'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploration des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la duree de vie des moteurs\n",
    "engine_life = train_df.groupby('unit_id')['cycle'].max()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(engine_life, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Duree de vie (cycles)')\n",
    "axes[0].set_ylabel('Nombre de moteurs')\n",
    "axes[0].set_title('Distribution de la duree de vie des moteurs')\n",
    "axes[0].axvline(engine_life.mean(), color='red', linestyle='--', label=f'Moyenne: {engine_life.mean():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].boxplot(engine_life)\n",
    "axes[1].set_ylabel('Duree de vie (cycles)')\n",
    "axes[1].set_title('Boxplot de la duree de vie')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Duree de vie moyenne: {engine_life.mean():.1f} cycles\")\n",
    "print(f\"Duree de vie min: {engine_life.min()} cycles\")\n",
    "print(f\"Duree de vie max: {engine_life.max()} cycles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier les valeurs manquantes\n",
    "print(\"Valeurs manquantes par colonne:\")\n",
    "print(train_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse des capteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance des capteurs - identifier ceux qui ont peu de variation\n",
    "sensor_cols = [f'sensor_{i}' for i in range(1, 22)]\n",
    "sensor_variance = train_df[sensor_cols].var().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sensor_variance.plot(kind='bar')\n",
    "plt.title('Variance des capteurs')\n",
    "plt.xlabel('Capteur')\n",
    "plt.ylabel('Variance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Capteurs avec variance proche de zero (peu utiles)\n",
    "low_variance = sensor_variance[sensor_variance < 0.0001]\n",
    "print(f\"\\nCapteurs avec variance quasi-nulle: {list(low_variance.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation des capteurs avec le RUL\n",
    "correlation_with_rul = train_df[sensor_cols + ['RUL']].corr()['RUL'].drop('RUL').sort_values()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "correlation_with_rul.plot(kind='barh', color=['red' if x < 0 else 'green' for x in correlation_with_rul])\n",
    "plt.title('Correlation des capteurs avec le RUL')\n",
    "plt.xlabel('Coefficient de correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 capteurs les plus correles (positivement):\")\n",
    "print(correlation_with_rul.tail(5))\n",
    "print(\"\\nTop 5 capteurs les plus correles (negativement):\")\n",
    "print(correlation_with_rul.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution des capteurs pour un moteur specifique\n",
    "engine_id = 1\n",
    "engine_data = train_df[train_df['unit_id'] == engine_id]\n",
    "\n",
    "# Selectionner les capteurs les plus correles avec le RUL\n",
    "top_sensors = correlation_with_rul.abs().sort_values(ascending=False).head(6).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, sensor in enumerate(top_sensors):\n",
    "    axes[i].plot(engine_data['cycle'], engine_data[sensor])\n",
    "    axes[i].set_xlabel('Cycle')\n",
    "    axes[i].set_ylabel(sensor)\n",
    "    axes[i].set_title(f'{sensor} - Moteur {engine_id}')\n",
    "\n",
    "plt.suptitle('Evolution des capteurs les plus correles avec le RUL', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer l'evolution de plusieurs moteurs\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "sensors_to_plot = ['sensor_2', 'sensor_4', 'sensor_11', 'sensor_15']\n",
    "engines_to_plot = [1, 10, 50, 80]\n",
    "\n",
    "for idx, sensor in enumerate(sensors_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    for engine in engines_to_plot:\n",
    "        data = train_df[train_df['unit_id'] == engine]\n",
    "        ax.plot(data['cycle'], data[sensor], label=f'Moteur {engine}', alpha=0.7)\n",
    "    ax.set_xlabel('Cycle')\n",
    "    ax.set_ylabel(sensor)\n",
    "    ax.set_title(f'Evolution de {sensor}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparation des donnees pour la modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les capteurs avec variance quasi-nulle\n",
    "cols_to_drop = ['sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
    "\n",
    "# Colonnes des features\n",
    "feature_cols = ['op_setting_1', 'op_setting_2', 'op_setting_3'] + \\\n",
    "               [col for col in sensor_cols if col not in cols_to_drop]\n",
    "\n",
    "print(f\"Features selectionnees ({len(feature_cols)}): {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer un plafond au RUL (technique courante pour ce dataset)\n",
    "# Les moteurs loin de la panne ont un RUL \"sature\" car leur degradation n'est pas encore visible\n",
    "RUL_CAP = 125\n",
    "\n",
    "train_df['RUL_capped'] = train_df['RUL'].clip(upper=RUL_CAP)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_df['RUL'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution RUL original')\n",
    "plt.xlabel('RUL')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(train_df['RUL_capped'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title(f'Distribution RUL plafonne (cap={RUL_CAP})')\n",
    "plt.xlabel('RUL')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des features\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = train_df[feature_cols].values\n",
    "y = train_df['RUL_capped'].values\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelisation - Modeles de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Entraine et evalue un modele\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    results = {\n",
    "        'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'Val RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "        'Train MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "        'Val MAE': mean_absolute_error(y_val, y_val_pred),\n",
    "        'Train R2': r2_score(y_train, y_train_pred),\n",
    "        'Val R2': r2_score(y_val, y_val_pred)\n",
    "    }\n",
    "    \n",
    "    return results, y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester plusieurs modeles\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    results, y_pred = evaluate_model(model, X_train, y_train, X_val, y_val)\n",
    "    results_df[name] = results\n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "results_df = results_df.T\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Resultats:\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les predictions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (name, y_pred) in enumerate(predictions.items()):\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(y_val, y_pred, alpha=0.3, s=10)\n",
    "    ax.plot([0, RUL_CAP], [0, RUL_CAP], 'r--', label='Prediction parfaite')\n",
    "    ax.set_xlabel('RUL reel')\n",
    "    ax.set_ylabel('RUL predit')\n",
    "    ax.set_title(f'{name}\\nRMSE: {results_df.loc[name, \"Val RMSE\"]:.2f}')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, RUL_CAP + 10)\n",
    "    ax.set_ylim(0, RUL_CAP + 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features (Random Forest)\n",
    "rf_model = models['Random Forest']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Importance des features (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ]
}